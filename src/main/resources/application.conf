
database {
  driver = "com.mysql.jdbc.Driver"
  host = "localhost"
  host = ${?DATABASE_HOST}
  username = "root"
  username = ${?DATABASE_USERNAME}
  password = ""
  password = ${?DATABASE_PASSWORD}
}

// We use this dispatcher to prevent actor starvation for our cluster heartbeat.
cluster-dispatcher {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-max = 4
  }
  throughput = 16
}

akka {
  loglevel = "DEBUG"
  coordinated-shutdown.terminate-actor-system = on

  actor {
    warn-about-java-serializer-usage = false
    provider = "akka.cluster.ClusterActorRefProvider"
    debug {
      lifecycle = on
      unhandled = on
    }
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = "127.0.0.1"
      hostname = ${?AKKA_REMOTING_BIND_HOST}
      port = 2551
      port = ${?AKKA_REMOTING_BIND_PORT}
    }
  }

  // DNS settings we override in our pods.
  io.dns.resolver = ${?DNS_RESOLVER}
  io.dns.async-dns.resolve-srv = ${?RESOLVE_SRV}
  io.dns.async-dns.resolv-conf = ${?RESOLV_CONF}

  cluster {
    use-dispatcher = cluster-dispatcher
    roles = ["manage", "compute"]
    roles = ${?AKKA_CLUSTER_ROLES}
    seed-nodes = ["akka.tcp://sclr@127.0.0.1:2551"]
    seed-nodes = ${?AKKA_SEED_NODES}
    //    # auto downing is NOT safe for production deployments.
    //    auto-down-unreachable-after = 10s

    pub-sub {
      send-to-dead-letters-when-no-subscribers = on
    }

    # Disable legacy metrics in akka-cluster.
    metrics {
      enabled = off
    }
    log-info = on
  }

  stream.materializer {
    # Initial size of buffers used in stream elements
    initial-input-buffer-size = 1
    # Maximum size of buffers used in stream elements
    max-input-buffer-size = 1

    stream-ref {
      # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref.
      # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,
      # because the delay of requesting over network boundaries is much higher.
      buffer-capacity = 1

      # Demand is signalled by sending a cumulative demand message ("requesting messages until the n-th sequence number)
      # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should
      # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).
      #
      # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.
      #
      # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive
      # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.
      demand-redelivery-interval = 1 second

      # Subscription timeout, during which the "remote side" MUST subscribe (materialize) the handed out stream ref.
      # This timeout does not have to be very low in normal situations, since the remote side may also need to
      # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking
      # in-active streams which are never subscribed to.
      subscription-timeout = 30 seconds
    }
  }

  http.server {
    default-http-host = "127.0.0.1"
    default-http-host = ${?HTTP_HOST}
    default-http-port = 8080
    default-http-port = ${?HTTP_PORT}
    default-https-port = 443
    default-https-port = ${?HTTPS_PORT}
  }

  extensions = ["akka.cluster.pubsub.DistributedPubSub"]
  //  extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]
  //  extensions = ["akka.cluster.metrics.ClusterMetricsExtension", "akka.cluster.pubsub.DistributedPubSub"]

}
